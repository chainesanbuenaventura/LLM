{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "382df0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, ConversationChain\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.document_loaders import GoogleDriveLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from config import *\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "sys.path.append('./chat')\n",
    "\n",
    "SALES_GOAL = \"\"\"provide clear and concise answers to potential customer about his queries about the company, but ultimately convince him to signup.\"\"\"\n",
    "\n",
    "SALES_TEMPLATE = \"\"\"You're role is customer service support. \\\n",
    "    But don't explicitly say you're trying to speak with this tone, just make sure you always do. \\\n",
    "    \n",
    "    Below is a query from a user and\n",
    "    some relevant contexts. Answer the question given the information in those\n",
    "    contexts. \n",
    "\n",
    "    Content Context: `{context}`\n",
    "\n",
    "    Conversation Handling:\n",
    "\n",
    "    Previous Chat: `{chat_history}`\n",
    "\n",
    "    Last Customer Message: `{human_input}`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = ChatPromptTemplate.from_template(SALES_TEMPLATE)\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, folder_id=None):\n",
    "#         if len(examples) > 0:\n",
    "#             self.examples_str = \"\\n\\n\".join([example['content'] for example in examples])\n",
    "#         else:\n",
    "#             self.examples_str = \"\"\n",
    "        # Load docs from Google docs\n",
    "        if folder_id is not None:\n",
    "            self.folder_id = folder_id\n",
    "            print(\"B\")\n",
    "            # Load docs from Google drive\n",
    "            self.load_docs_from_google()\n",
    "            print(\"C\")\n",
    "        else:\n",
    "            self.docs_google = []\n",
    "        # Load docs from manual docs on studio\n",
    "        self.load_docs_from_json()\n",
    "        # Contatenate all docs\n",
    "        self.docs = self.docs_json + self.docs_google\n",
    "\n",
    "        # Create Chroma vectorstore\n",
    "        self.create_vectorstore()\n",
    "        print(\"D\")\n",
    "\n",
    "        # Define qa chain parameters\n",
    "        self.llm = ChatOpenAI(temperature=0)\n",
    "        print(\"E\")\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "        print(\"F\")\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \n",
    "                             \"human_input\", \n",
    "                             \"context\" ], template=SALES_TEMPLATE\n",
    "            )\n",
    "        print(\"G\")\n",
    "        self.qachain = load_qa_chain(\n",
    "            self.llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=self.memory, \n",
    "            prompt=self.prompt\n",
    "        )\n",
    "        print(\"H\")\n",
    "\n",
    "    def get_response_over_docs(self, query):\n",
    "        relevant_docs = self.retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "        return self.qachain(\n",
    "            {\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "    \n",
    "    def load_docs_from_json(self):\n",
    "        \"\"\"Load docs from manual input on studio\"\"\"\n",
    "        # Document loading\n",
    "        self.loader_json = JSONLoader(\n",
    "            file_path=f'./docs/{self.sessionId}.json',\n",
    "            jq_schema='.documents[].description')\n",
    "\n",
    "        self.docs_json = self.loader_json.load()\n",
    "\n",
    "    def load_docs_from_google(self):\n",
    "        \"\"\"Folder should contain Google docs\"\"\"\n",
    "        try:\n",
    "            # Document loading\n",
    "            self.loader_google = GoogleDriveLoader(\n",
    "                folder_id=self.folder_id,\n",
    "                # Optional: configure whether to recursively fetch files from subfolders. Defaults to False.\n",
    "                recursive=False,\n",
    "                credentials_path=\".credentials/credentials.json\",\n",
    "                token_path=\".credentials/token.json\",\n",
    "            )\n",
    "\n",
    "            self.docs_google = self.loader_google.load()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Check if the error relates to token expiration (adapt based on actual exception message)\n",
    "            if 'token expired' in str(e).lower():\n",
    "                refreshed = self.refresh_token()\n",
    "                if refreshed:\n",
    "                    self.load_docs_from_google() # Retry loading the documents\n",
    "                else:\n",
    "                    raise Exception(\"Failed to refresh the token.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        self.documents = self.text_splitter.split_documents(self.docs)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectorstore = Chroma.from_documents(self.documents, self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\":1})\n",
    "    \n",
    "    def refresh_token(self):\n",
    "        # Load values from credentials.json\n",
    "        with open('.credentials/credentials.json', 'r') as cred_file:\n",
    "            cred_data = json.load(cred_file)\n",
    "        \n",
    "        # Load refresh_token from token.json\n",
    "        with open('.credentials/token.json', 'r') as token_file:\n",
    "            token_data = json.load(token_file)\n",
    "\n",
    "        # Constructing the refresh URL with the necessary parameters\n",
    "        params = {\n",
    "            'client_id': cred_data['installed']['client_id'],\n",
    "            'client_secret': cred_data['installed']['client_secret'],\n",
    "            'refresh_token': token_data['refresh_token'],\n",
    "            'grant_type': 'refresh_token'\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"https://oauth2.googleapis.com/token\", data=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            new_token = response.json()['access_token']\n",
    "            \n",
    "            # Load existing token.json into a dictionary\n",
    "            with open('.credentials/token.json', 'r') as token_file:\n",
    "                token_data = json.load(token_file)\n",
    "            \n",
    "            # Update the access token\n",
    "            token_data['token'] = new_token\n",
    "            \n",
    "            # Save updated token.json\n",
    "            with open('.credentials/token.json', 'w') as token_file:\n",
    "                json.dump(token_data, token_file)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Error refreshing token:\", response.json())\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8a3ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 528, which is longer than the specified 500\n",
      "Parameter temperature does not exist for OpenAIChat\n",
      "Parameter temperature does not exist for OpenAIChat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided in the content context, the employment situation in various major industries remained relatively stable over the month. There was little change in employment in industries such as mining, quarrying, and oil and gas extraction; construction; manufacturing; wholesale trade; information; financial activities; and other services.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from elevenlabs import set_api_key\n",
    "# import requests\n",
    "\n",
    "# from chat import QAChain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "# Greetings and obtaining the user's name\n",
    "define user greet\n",
    "  \"Hello\"\n",
    "  \"Hi\"\n",
    "  \"Hey\"\n",
    "  \"Good day\"\n",
    "\n",
    "define bot greet_ask_name\n",
    "  \"Hello! How can I assist you today? By the way, may I have your name?\"\n",
    "\n",
    "define user provide_name\n",
    "  \"My name is [name]\"\n",
    "  \"I'm [name]\"\n",
    "  \"[name]\"\n",
    "\n",
    "define bot acknowledge_name\n",
    "  \"Nice to meet you, [name]! How can I help you today?\"\n",
    "\n",
    "# Handling common customer inquiries\n",
    "define user ask_return_policy\n",
    "  \"What's your return policy?\"\n",
    "  \"How do I return an item?\"\n",
    "  \"Can I return this?\"\n",
    "\n",
    "define bot explain_return_policy\n",
    "  \"Our return policy allows for returns within 30 days of purchase. Ensure the item is unused and in its original packaging. Would you like help with anything else, [name]?\"\n",
    "\n",
    "define user ask_shipping_status\n",
    "  \"Where's my order?\"\n",
    "  \"Has my item shipped?\"\n",
    "  \"What's the status of my delivery?\"\n",
    "\n",
    "define bot explain_shipping_status\n",
    "  \"I can definitely help you with that. Can you provide me with your order number or the email address you used for the purchase?\"\n",
    "\n",
    "# Ensuring conversation remains on topic\n",
    "define user off_topic\n",
    "  \"What's your favorite color?\"\n",
    "  \"Do you like movies?\"\n",
    "  \"How's the weather?\"\n",
    "\n",
    "define bot steer_back_to_topic\n",
    "  \"I'm here to help with customer service inquiries. How can I assist you with your order or any other service-related question?\"\n",
    "\n",
    "# Encouraging natural conversation\n",
    "define user thanks\n",
    "  \"Thank you\"\n",
    "  \"Thanks\"\n",
    "  \"I appreciate your help\"\n",
    "\n",
    "define bot respond_thanks\n",
    "  \"You're welcome, [name]! If you have any more questions, feel free to ask. Have a great day!\"\n",
    "\n",
    "# Flows to connect user and bot messages\n",
    "define flow\n",
    "  user greet\n",
    "  bot greet_ask_name\n",
    "\n",
    "define flow\n",
    "  user provide_name\n",
    "  bot acknowledge_name\n",
    "\n",
    "define flow\n",
    "  user ask_return_policy\n",
    "  bot explain_return_policy\n",
    "\n",
    "define flow\n",
    "  user ask_shipping_status\n",
    "  bot explain_shipping_status\n",
    "\n",
    "define flow\n",
    "  user off_topic\n",
    "  bot steer_back_to_topic\n",
    "\n",
    "define flow\n",
    "  user thanks\n",
    "  bot respond_thanks\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "instructions:\n",
    "- type: general\n",
    "  content: |\n",
    "    Below is a conversation between a bot and a user. The bot is here to provide customer support for a company. It should remain on the topic of the company's products and services. The bot should ask the user's name and converse in a natural manner without delving into unrelated or strange topics.\n",
    "\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: gpt-3.5-turbo\n",
    "\n",
    "sample_conversation: |\n",
    "  user: \"Hello!\"\n",
    "  bot: \"Hello! How can I assist you with our products or services today? May I know your name?\"\n",
    "  user: \"I'm Alex. I have a question about your product.\"\n",
    "  bot: \"Sure, Alex! Please go ahead with your question.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Initializes a QA chain using the jobs report.\n",
    "\n",
    "It uses OpenAI embeddings.\n",
    "\"\"\"\n",
    "loader = TextLoader(\n",
    "        \"NeMo-Guardrails/examples/grounding_rail/kb/report.md\",\n",
    "    )\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \n",
    "                     \"human_input\", \n",
    "                     \"context\" ], template=SALES_TEMPLATE)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":1})\n",
    "\n",
    "\"\"\"Demo of using a chain as a custom action.\"\"\"\n",
    "config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "\n",
    "qa_chain = load_qa_chain(\n",
    "            app.llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=memory, \n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "query = \"Summarize\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "response = qa_chain({\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "start_time = time.time()\n",
    "app.register_action(qa_chain, name=\"qa_chain\")\n",
    "\n",
    "# Change to mode here to experiment with the multiple ways of using the chain\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the current unemployment rate?\"}\n",
    "]\n",
    "result = app.generate(messages=history)\n",
    "app.register_action(qa_chain, name=\"qa_chain\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0623ddb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, the content context does not mention the location of France. Could you please provide additional information or clarify your question?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Where is France\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "response = qa_chain({\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7920eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
