{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea52404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, ConversationChain, LLMChain\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.document_loaders import GoogleDriveLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory, ConversationBufferMemory\n",
    "# from config import *\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "sys.path.append('./chat')\n",
    "\n",
    "# SALES_GOAL = \"\"\"provide clear and concise answers to potential customer about his queries about the company, but ultimately convince him to signup.\"\"\"\n",
    "\n",
    "SALES_TEMPLATE = \"\"\"As acustomer service support, \\\n",
    "    respond to only questions to questions related to the Employment Situation report. If \"Content Context\" is empty, say that you don't know. \\\n",
    "    \n",
    "    Below is a query from a user and\n",
    "    some relevant contexts. Answer the question given the information in those\n",
    "    contexts. \n",
    "\n",
    "    `Content Context`: ```{context}```\n",
    "\n",
    "    Conversation Handling:\n",
    "\n",
    "    Previous Chat: `{chat_history}`\n",
    "\n",
    "    Last Customer Message: `{human_input}`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = ChatPromptTemplate.from_template(SALES_TEMPLATE)\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, folder_id=None):\n",
    "#         if len(examples) > 0:\n",
    "#             self.examples_str = \"\\n\\n\".join([example['content'] for example in examples])\n",
    "#         else:\n",
    "#             self.examples_str = \"\"\n",
    "        # Load docs from Google docs\n",
    "        if folder_id is not None:\n",
    "            self.folder_id = folder_id\n",
    "            # Load docs from Google drive\n",
    "            self.load_docs_from_google()\n",
    "        else:\n",
    "            self.docs_google = []\n",
    "        # Load docs from manual docs on studio\n",
    "#         self.load_docs_from_json()\n",
    "        # Contatenate all docs\n",
    "        self.docs = TextLoader(\n",
    "        \"NeMo-Guardrails/examples/grounding_rail/kb/report.md\",\n",
    "    ).load()\n",
    "\n",
    "        # Create Chroma vectorstore\n",
    "        self.create_vectorstore()\n",
    "\n",
    "        # Define qa chain parameters\n",
    "        self.llm = ChatOpenAI(temperature=0)\n",
    "        self.memory = ConversationSummaryBufferMemory(llm=self.llm, memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \n",
    "                             \"human_input\", \n",
    "                             \"context\" ], template=SALES_TEMPLATE\n",
    "            )\n",
    "        self.qa_chain = load_qa_chain(\n",
    "            self.llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=self.memory, \n",
    "            prompt=self.prompt\n",
    "        )\n",
    "        \n",
    "        self.retrieval_chain = TransformChain(\n",
    "                    input_variables=[\"question\"],\n",
    "                    output_variables=[\"human_input\", \"input_documents\"],\n",
    "                    transform=self.retrieval_transform\n",
    "                )\n",
    "\n",
    "        self.rag_chain = SequentialChain(\n",
    "            chains=[self.retrieval_chain, self.qa_chain],\n",
    "            input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "            output_variables=[\"output_text\"]\n",
    "        )\n",
    "        \n",
    "    def retrieval_transform(inputs: dict) -> dict:\n",
    "        docs = retriever.get_relevant_documents(query=inputs[\"question\"])   \n",
    "    #     docs = [d.page_content for d in docs]\n",
    "        docs_dict = {\n",
    "            \"human_input\": inputs[\"question\"],\n",
    "            \"input_documents\": docs\n",
    "        }\n",
    "        return docs_dict\n",
    "    \n",
    "    def get_response_over_docs(self, query):\n",
    "        relevant_docs = self.retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "        return self.qa_chain(\n",
    "            {\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "    def get_response_over_docs2(self, query):\n",
    "        out = rag_chain({\"question\": query})\n",
    "        return out[\"output_text\"]\n",
    "    \n",
    "    def load_docs_from_json(self):\n",
    "        \"\"\"Load docs from manual input on studio\"\"\"\n",
    "        # Document loading\n",
    "        self.loader_json = JSONLoader(\n",
    "            file_path=f'./docs/{self.sessionId}.json',\n",
    "            jq_schema='.documents[].description')\n",
    "\n",
    "        self.docs_json = self.loader_json.load()\n",
    "\n",
    "    def load_docs_from_google(self):\n",
    "        \"\"\"Folder should contain Google docs\"\"\"\n",
    "        try:\n",
    "            # Document loading\n",
    "            self.loader_google = GoogleDriveLoader(\n",
    "                folder_id=self.folder_id,\n",
    "                # Optional: configure whether to recursively fetch files from subfolders. Defaults to False.\n",
    "                recursive=False,\n",
    "                credentials_path=\".credentials/credentials.json\",\n",
    "                token_path=\".credentials/token.json\",\n",
    "            )\n",
    "\n",
    "            self.docs_google = self.loader_google.load()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Check if the error relates to token expiration (adapt based on actual exception message)\n",
    "            if 'token expired' in str(e).lower():\n",
    "                refreshed = self.refresh_token()\n",
    "                if refreshed:\n",
    "                    self.load_docs_from_google() # Retry loading the documents\n",
    "                else:\n",
    "                    raise Exception(\"Failed to refresh the token.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        self.documents = self.text_splitter.split_documents(self.docs)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectorstore = Chroma.from_documents(self.documents, self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\":1})\n",
    "    \n",
    "    def refresh_token(self):\n",
    "        # Load values from credentials.json\n",
    "        with open('.credentials/credentials.json', 'r') as cred_file:\n",
    "            cred_data = json.load(cred_file)\n",
    "        \n",
    "        # Load refresh_token from token.json\n",
    "        with open('.credentials/token.json', 'r') as token_file:\n",
    "            token_data = json.load(token_file)\n",
    "\n",
    "        # Constructing the refresh URL with the necessary parameters\n",
    "        params = {\n",
    "            'client_id': cred_data['installed']['client_id'],\n",
    "            'client_secret': cred_data['installed']['client_secret'],\n",
    "            'refresh_token': token_data['refresh_token'],\n",
    "            'grant_type': 'refresh_token'\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"https://oauth2.googleapis.com/token\", data=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            new_token = response.json()['access_token']\n",
    "            \n",
    "            # Load existing token.json into a dictionary\n",
    "            with open('.credentials/token.json', 'r') as token_file:\n",
    "                token_data = json.load(token_file)\n",
    "            \n",
    "            # Update the access token\n",
    "            token_data['token'] = new_token\n",
    "            \n",
    "            # Save updated token.json\n",
    "            with open('.credentials/token.json', 'w') as token_file:\n",
    "                json.dump(token_data, token_file)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Error refreshing token:\", response.json())\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d32e1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 528, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "AI: I'm sorry, but the query you provided is not related to the Employment Situation report. If you have any questions or concerns regarding the report, I'll be happy to assist you.\n",
      "2.000540614128113\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# import requests\n",
    "\n",
    "# from chat import QAChain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.chains import TransformChain, SequentialChain\n",
    "\n",
    "OPENAI_API_KEY = \n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "\"\"\"Initializes a QA chain using the jobs report.\n",
    "\n",
    "It uses OpenAI embeddings.\n",
    "\"\"\"\n",
    "loader = TextLoader(\n",
    "        \"NeMo-Guardrails/examples/grounding_rail/kb/report.md\",\n",
    "    )\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \n",
    "                     \"human_input\", \n",
    "                     \"context\" ], template=SALES_TEMPLATE)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":1})\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])   \n",
    "#     docs = [d.page_content for d in docs]\n",
    "    docs_dict = {\n",
    "        \"human_input\": inputs[\"question\"],\n",
    "        \"input_documents\": docs\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "                    input_variables=[\"question\"],\n",
    "                    output_variables=[\"human_input\", \"input_documents\"],\n",
    "                    transform=retrieval_transform\n",
    "                )\n",
    "\n",
    "qa_chain = load_qa_chain(\n",
    "            llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=memory, \n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"output_text\"]\n",
    ")\n",
    "\n",
    "t = 0\n",
    "\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "\n",
    "    out = rag_chain({\"question\": \"Where is Germany\"})\n",
    "    print(out[\"output_text\"])\n",
    "\n",
    "    t += time.time() - start\n",
    "    \n",
    "print(t/10)\n",
    "\n",
    "# query = \"Summarize\"\n",
    "\n",
    "# relevant_docs = retriever.get_relevant_documents(\n",
    "#             query\n",
    "#         )\n",
    "\n",
    "# response = qa_chain({\"input_documents\": relevant_docs, \n",
    "#              \"human_input\": query}, \n",
    "#             return_only_outputs=True)['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4f9e06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 528, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "Germany is a country located in Europe.\n",
      "1.0837165117263794\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Germany\"\n",
    "\n",
    "qa_chain = QAChain()\n",
    "\n",
    "t = 0\n",
    "\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "\n",
    "    response = qa_chain.get_response_over_docs2(query)\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    t += time.time() - start\n",
    "    \n",
    "print(t/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df5249c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 528, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "AI: I'm sorry, but I don't have any information about the employment situation in the Milky Way. My knowledge is limited to the Employment Situation report for the United States.\n",
      "1.7093297004699708\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Employment situation in milky way\"\n",
    "\n",
    "qa_chain = QAChain()\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "t = 0\n",
    "\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "\n",
    "    response = qa_chain.get_response_over_docs(query)\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    t += time.time() - start\n",
    "    \n",
    "print(t/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4843a0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent stopped due to iteration limit or time limit.\n",
      "10.21777892112732\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.initialize import initialize_agent\n",
    "from langchain.tools import Tool\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":1}, return_sources=False)\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"retrieval\",\n",
    "        func=lambda q: str(retriever.get_relevant_documents(\n",
    "            query\n",
    "        )),\n",
    "        description=\"useful only for when you want to answer questions about the release of the Employment Situation report for April\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "query = \"What is the unemployment rate?\"\n",
    "\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \n",
    "                     \"human_input\", \n",
    "                     \"context\" ], template=SALES_TEMPLATE\n",
    "    )\n",
    "agent_executor2 = initialize_agent(\n",
    "    tools, llm, agent=\"conversational-react-description\", memory=memory, handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "print(agent_executor2.run(input=query))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d18d572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tool??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52fdb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France is a country located in Western Europe. It is bordered by Belgium, Luxembourg, Germany, Switzerland, Italy, Spain, Andorra, and Monaco. The capital city of France is Paris, which is known for its iconic landmarks such as the Eiffel Tower and the Louvre Museum. France is also famous for its cuisine, fashion, art, and culture.\n",
      "2.5997068881988525\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from llama_index import VectorStoreIndex, WikipediaReader\n",
    "documents = WikipediaReader().load_data(pages=[\"Covid-19\"])\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: str(index.as_query_engine().query(q)),\n",
    "        description=\"useful for when you want to answer questions about covid\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "]\n",
    "# set Logging to DEBUG for more detailed outputs\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "agent_executor = initialize_agent(\n",
    "    tools, llm, agent=\"conversational-react-description\", memory=memory, handle_parsing_errors=True\n",
    ")\n",
    "start = time.time()\n",
    "print(agent_executor.run(input=\"where is france\"))\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f2cef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
