{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07761704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.db.duckdb:Exiting: Cleaning up .chroma directory\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain, ConversationChain\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.document_loaders import GoogleDriveLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "# from config import *\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "sys.path.append('./chat')\n",
    "\n",
    "SALES_GOAL = \"\"\"provide clear and concise answers to potential customer about his queries about the company, but ultimately convince him to signup.\"\"\"\n",
    "\n",
    "SALES_TEMPLATE = \"\"\"\n",
    "    Below is a query from a user and\n",
    "    some relevant contexts. Answer the question given the information in those\n",
    "    contexts. \n",
    "\n",
    "    Content Context: `{context}`\n",
    "\n",
    "    Conversation Handling:\n",
    "\n",
    "    Previous Chat: `{chat_history}`\n",
    "\n",
    "    Last Customer Message: `{human_input}`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = ChatPromptTemplate.from_template(SALES_TEMPLATE)\n",
    "\n",
    "class QAChain:\n",
    "    def __init__(self, folder_id=None):\n",
    "#         if len(examples) > 0:\n",
    "#             self.examples_str = \"\\n\\n\".join([example['content'] for example in examples])\n",
    "#         else:\n",
    "#             self.examples_str = \"\"\n",
    "        # Load docs from Google docs\n",
    "        if folder_id is not None:\n",
    "            self.folder_id = folder_id\n",
    "            print(\"B\")\n",
    "            # Load docs from Google drive\n",
    "            self.load_docs_from_google()\n",
    "            print(\"C\")\n",
    "        else:\n",
    "            self.docs_google = []\n",
    "        # Load docs from manual docs on studio\n",
    "        self.load_docs_from_json()\n",
    "        # Contatenate all docs\n",
    "        self.docs = self.docs_json + self.docs_google\n",
    "\n",
    "        # Create Chroma vectorstore\n",
    "        self.create_vectorstore()\n",
    "        print(\"D\")\n",
    "\n",
    "        # Define qa chain parameters\n",
    "        self.llm = ChatOpenAI(temperature=0)\n",
    "        print(\"E\")\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "        print(\"F\")\n",
    "        self.prompt = PromptTemplate(\n",
    "            input_variables=[\"chat_history\", \n",
    "                             \"human_input\", \n",
    "                             \"context\" ], template=SALES_TEMPLATE\n",
    "            )\n",
    "        print(\"G\")\n",
    "        self.qachain = load_qa_chain(\n",
    "            self.llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=self.memory, \n",
    "            prompt=self.prompt\n",
    "        )\n",
    "        print(\"H\")\n",
    "\n",
    "    def get_response_over_docs(self, query):\n",
    "        relevant_docs = self.retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "        return self.qachain(\n",
    "            {\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "    \n",
    "    def load_docs_from_json(self):\n",
    "        \"\"\"Load docs from manual input on studio\"\"\"\n",
    "        # Document loading\n",
    "        self.loader_json = JSONLoader(\n",
    "            file_path=f'./docs/{self.sessionId}.json',\n",
    "            jq_schema='.documents[].description')\n",
    "\n",
    "        self.docs_json = self.loader_json.load()\n",
    "\n",
    "    def load_docs_from_google(self):\n",
    "        \"\"\"Folder should contain Google docs\"\"\"\n",
    "        try:\n",
    "            # Document loading\n",
    "            self.loader_google = GoogleDriveLoader(\n",
    "                folder_id=self.folder_id,\n",
    "                # Optional: configure whether to recursively fetch files from subfolders. Defaults to False.\n",
    "                recursive=False,\n",
    "                credentials_path=\".credentials/credentials.json\",\n",
    "                token_path=\".credentials/token.json\",\n",
    "            )\n",
    "\n",
    "            self.docs_google = self.loader_google.load()\n",
    "\n",
    "        except Exception as e:\n",
    "            # Check if the error relates to token expiration (adapt based on actual exception message)\n",
    "            if 'token expired' in str(e).lower():\n",
    "                refreshed = self.refresh_token()\n",
    "                if refreshed:\n",
    "                    self.load_docs_from_google() # Retry loading the documents\n",
    "                else:\n",
    "                    raise Exception(\"Failed to refresh the token.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "    def create_vectorstore(self):\n",
    "        self.text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "        self.documents = self.text_splitter.split_documents(self.docs)\n",
    "\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.vectorstore = Chroma.from_documents(self.documents, self.embeddings)\n",
    "        self.retriever = self.vectorstore.as_retriever(search_kwargs={\"k\":1})\n",
    "    \n",
    "    def refresh_token(self):\n",
    "        # Load values from credentials.json\n",
    "        with open('.credentials/credentials.json', 'r') as cred_file:\n",
    "            cred_data = json.load(cred_file)\n",
    "        \n",
    "        # Load refresh_token from token.json\n",
    "        with open('.credentials/token.json', 'r') as token_file:\n",
    "            token_data = json.load(token_file)\n",
    "\n",
    "        # Constructing the refresh URL with the necessary parameters\n",
    "        params = {\n",
    "            'client_id': cred_data['installed']['client_id'],\n",
    "            'client_secret': cred_data['installed']['client_secret'],\n",
    "            'refresh_token': token_data['refresh_token'],\n",
    "            'grant_type': 'refresh_token'\n",
    "        }\n",
    "\n",
    "        response = requests.post(\"https://oauth2.googleapis.com/token\", data=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            new_token = response.json()['access_token']\n",
    "            \n",
    "            # Load existing token.json into a dictionary\n",
    "            with open('.credentials/token.json', 'r') as token_file:\n",
    "                token_data = json.load(token_file)\n",
    "            \n",
    "            # Update the access token\n",
    "            token_data['token'] = new_token\n",
    "            \n",
    "            # Save updated token.json\n",
    "            with open('.credentials/token.json', 'w') as token_file:\n",
    "                json.dump(token_data, token_file)\n",
    "\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Error refreshing token:\", response.json())\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a14fd4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.text_splitter:Created a chunk of size 528, which is longer than the specified 500\n",
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:backoff:Backing off send_request(...) for 0.1s (requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')))\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Initializing action dispatcher\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding retrieve_relevant_chunks to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding check_jailbreak to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding output_moderation_v2 to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding output_moderation to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding wolfram_alpha_request to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Added summarize_document to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Registered Actions: {'retrieve_relevant_chunks': <function retrieve_relevant_chunks at 0x7ff408c32160>, 'check_jailbreak': <function check_jailbreak at 0x7ff408e11550>, 'output_moderation_v2': <function output_moderation_v2 at 0x7ff408e11430>, 'output_moderation': <function output_moderation at 0x7ff408e11670>, 'wolfram alpha request': <function wolfram_alpha_request at 0x7ff408e115e0>, 'summarize_document': <class 'summarize_document.py.SummarizeDocument'>}\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Action dispatcher initialized\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f2cc339fb643ce826aed3a91bf2362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a453c1af58b04c97b74f92a9b214d6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f357d218fe24cca81cee92600a8e62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import base64\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from elevenlabs import set_api_key\n",
    "# import requests\n",
    "\n",
    "# from chat import QAChain\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \n",
    "\n",
    "# COLANG_CONFIG = \"\"\"\n",
    "# # Greetings\n",
    "# define user greet\n",
    "#   \"Hello\"\n",
    "#   \"Hi\"\n",
    "#   \"Hey\"\n",
    "#   \"Good day\"\n",
    "\n",
    "# define bot greet\n",
    "#   \"Hello! How can I assist you with your customer service needs today?\"\n",
    "\n",
    "# # Obtaining the user's name\n",
    "# define user provide_name\n",
    "#   \"My name is [name]\"\n",
    "#   \"I'm [name]\"\n",
    "#   \"[name]\"\n",
    "\n",
    "# define bot acknowledge_name\n",
    "#   \"Thank you, [name]. How may I assist you further?\"\n",
    "\n",
    "# # Ensuring conversation remains on topic\n",
    "# define user off_topic\n",
    "#   \"What's your favorite color?\"\n",
    "#   \"Do you believe in aliens?\"\n",
    "#   \"How's the weather?\"\n",
    "\n",
    "# define bot steer_back_to_topic\n",
    "#   \"I'm here to help with customer service inquiries, [name]. Please let me know how I can assist you regarding your order or our services.\"\n",
    "\n",
    "# # Ending the conversation\n",
    "# define user end_conversation\n",
    "#   \"Thank you\"\n",
    "#   \"Thanks\"\n",
    "#   \"That's all I needed\"\n",
    "#   \"Goodbye\"\n",
    "\n",
    "# define bot end_response\n",
    "#   \"You're welcome, [name]! If you have any other questions in the future, don't hesitate to reach out. Have a great day!\"\n",
    "\n",
    "# # Flows\n",
    "# define flow\n",
    "#   user greet\n",
    "#   bot greet\n",
    "\n",
    "# define flow\n",
    "#   user provide_name\n",
    "#   bot acknowledge_name\n",
    "\n",
    "# define flow\n",
    "#   user ask_return_policy\n",
    "#   bot explain_return_policy\n",
    "\n",
    "# define flow\n",
    "#   user ask_shipping_status\n",
    "#   bot ask_order_details\n",
    "\n",
    "# define flow\n",
    "#   user off_topic\n",
    "#   bot steer_back_to_topic\n",
    "\n",
    "# define flow\n",
    "#   user end_conversation\n",
    "#   bot end_response\n",
    "  \n",
    "# # Catch-all for off-topic queries\n",
    "# define user off_topic_general\n",
    "#   \"*\"\n",
    "\n",
    "# define bot steer_back_to_general_topic\n",
    "#   \"I'm here primarily to help with customer service inquiries related to our products and services. How can I assist you with that?\"\n",
    "# \"\"\"\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user express greeting\n",
    "  \"Hello\"\n",
    "  \"Hi\"\n",
    "  \"Wassup?\"\n",
    "\n",
    "define bot express greeting\n",
    "  \"Hey there! What's your name?\"\n",
    "\n",
    "define bot ask how are you\n",
    "  \"How are you doing?\"\n",
    "  \"How's it going?\"\n",
    "  \"How are you feeling today?\"\n",
    "  \n",
    "define user express feeling good\n",
    "    \"I'm great\"\n",
    "    \n",
    "define bot express positive emotion\n",
    "    \"That's great\"\n",
    "    \n",
    "define user express feeling bad\n",
    "    \"I don't feel good\"\n",
    "    \n",
    "define bot express empathy\n",
    "    \"Sorry you feel that way\"\n",
    "\n",
    "define flow greeting\n",
    "  user express greeting\n",
    "  bot express greeting\n",
    "\n",
    "  bot ask how are you\n",
    "\n",
    "  when user express feeling good\n",
    "   bot express positive emotion\n",
    "\n",
    "  else when user express feeling bad\n",
    "   bot express empathy\n",
    "   \n",
    "define user ask about politics\n",
    "  \"What do you think about the government?\"\n",
    "  \"Which party should I vote for?\"\n",
    "\n",
    "define user ask about stock market\n",
    "  \"Which stock should I invest in?\"\n",
    "  \"Would this stock 10x over the next year?\"\n",
    "  \n",
    "define bot inform cannot respond\n",
    "  \"Sorry I cannot respond to that question\"\n",
    "  \n",
    "define flow politics\n",
    "  user ask about politics\n",
    "  bot inform cannot respond\n",
    "\n",
    "define flow stock market\n",
    "  user ask about stock market\n",
    "  bot inform cannot respond\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "instructions:\n",
    "- type: general\n",
    "  content: |\n",
    "    Below is a conversation between a bot and a user. If the bot does not know the answer to a\n",
    "    question, it truthfully says it does not know.\n",
    "\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: gpt-3.5-turbo\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# YAML_CONFIG = \"\"\"\n",
    "# instructions:\n",
    "# - type: general\n",
    "#   content: |\n",
    "#     Below is a conversation between a bot and a user. The bot is here to provide customer support for a company. It should remain on the topic of the company's products and services. The bot should ask the user's name and converse in a natural manner without delving into unrelated or strange topics.\n",
    "#     The bot is factual and concise. If the bot does not know the answer to a\n",
    "#     question, it truthfully says it does not know.\n",
    "\n",
    "# models:\n",
    "#   - type: main\n",
    "#     engine: openai\n",
    "#     model: gpt-3.5-turbo\n",
    "\n",
    "# sample_conversation: |\n",
    "#   user: \"Hello!\"\n",
    "#   bot: \"Hello! How can I assist you with our products or services today? May I know your name?\"\n",
    "#   user: \"I'm Alex. I have a question about your product.\"\n",
    "#   bot: \"Sure, Alex! Please go ahead with your question.\"\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\"\"\"Initializes a QA chain using the jobs report.\n",
    "\n",
    "It uses OpenAI embeddings.\n",
    "\"\"\"\n",
    "loader = TextLoader(\n",
    "        \"NeMo-Guardrails/examples/grounding_rail/kb/report.md\",\n",
    "    )\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", input_key=\"human_input\")\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \n",
    "                     \"human_input\", \n",
    "                     \"context\" ], template=SALES_TEMPLATE)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\":1})\n",
    "\n",
    "\"\"\"Demo of using a chain as a custom action.\"\"\"\n",
    "# print(COLANG_CONFIG)\n",
    "config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "\n",
    "qa_chain = load_qa_chain(\n",
    "            app.llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=memory, \n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "# query = \"What do you think about the government\"\n",
    "\n",
    "# relevant_docs = retriever.get_relevant_documents(\n",
    "#             query\n",
    "#         )\n",
    "\n",
    "# response = qa_chain({\"input_documents\": relevant_docs, \n",
    "#              \"human_input\": query}, \n",
    "#             return_only_outputs=True)['output_text']\n",
    "\n",
    "start_time = time.time()\n",
    "app.register_action(qa_chain, name=\"qa_chain\")\n",
    "\n",
    "# # Change to mode here to experiment with the multiple ways of using the chain\n",
    "# history = [\n",
    "#     {\"role\": \"user\", \"content\": \"What do you think about the government\"}\n",
    "# ]\n",
    "# result = app.generate(messages=history)\n",
    "# app.register_action(qa_chain, name=\"qa_chain\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "364e40a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I cannot provide personal opinions or endorse any political party. It ultimately depends on your own beliefs, values, and priorities. It is important to research and consider the platforms, policies, and candidates of each party before making a decision. You can also consider discussing with people you trust, as well as participating in local events or debates to gather more information.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which party should I vote for\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "response = qa_chain({\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "887c4495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, the content context does not mention the location of France. Could you please provide additional information or clarify your question?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Where is France\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "response = qa_chain({\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a23ccee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "response = qa_chain({\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f8ba215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nemoguardrails.actions.action_dispatcher:Initializing action dispatcher\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding retrieve_relevant_chunks to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding check_jailbreak to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding output_moderation_v2 to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding output_moderation to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Adding wolfram_alpha_request to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Added summarize_document to actions\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Registered Actions: {'retrieve_relevant_chunks': <function retrieve_relevant_chunks at 0x7ff40bfa35e0>, 'check_jailbreak': <function check_jailbreak at 0x7ff408e11c10>, 'output_moderation_v2': <function output_moderation_v2 at 0x7ff408e11ee0>, 'output_moderation': <function output_moderation at 0x7ff408e11a60>, 'wolfram alpha request': <function wolfram_alpha_request at 0x7ff408e11b80>, 'summarize_document': <class 'summarize_document.py.SummarizeDocument'>}\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Action dispatcher initialized\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6694fa00dd4b919b9a5168c6f356d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4f7a1899ed46919dbd345b7ab12f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9b421866be41a1936bf4638981c881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.posthog:Anonymized telemetry enabled. See https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Example of using a QnA chain with guardrails.\"\"\"\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "COLANG_CONFIG = \"\"\"\n",
    "define user express greeting\n",
    "  \"hi\"\n",
    "\n",
    "define user express insult\n",
    "  \"You are stupid\"\n",
    "\n",
    "# Basic guardrail against insults.\n",
    "define flow\n",
    "  user express insult\n",
    "  bot express calmly willingness to help\n",
    "\n",
    "# Here we use the QA chain for anything else.\n",
    "define flow\n",
    "  user ...\n",
    "  $answer = execute qa_chain(query=$last_user_message)\n",
    "  bot $answer\n",
    "  \n",
    "define user ask off topic\n",
    "    \"Who is the president?\"\n",
    "    \"Can you recommend the best stocks to buy?\"\n",
    "    \"Can you write an email?\"\n",
    "    \"Can you tell me a joke?\"\n",
    "    ...\n",
    "\n",
    "define bot explain cant help with off topic\n",
    "    \"I cannot comment on anything which is not relevant to the job report\"\n",
    "\n",
    "define flow\n",
    "    user ask off topic\n",
    "    bot explain cant help with off topic\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: text-davinci-003\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _get_qa_chain(llm):\n",
    "    \"\"\"Initializes a QA chain using the jobs report.\n",
    "\n",
    "    It uses OpenAI embeddings.\n",
    "    \"\"\"\n",
    "    loader = TextLoader(\n",
    "        \"NeMo-Guardrails/examples/grounding_rail/kb/report.md\",\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    docsearch = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "#     qa_chain = RetrievalQA.from_chain_type(\n",
    "#         llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever()\n",
    "#     )\n",
    "    qa_chain = load_qa_chain(\n",
    "            app.llm, \n",
    "            chain_type=\"stuff\", \n",
    "            memory=memory, \n",
    "            prompt=prompt\n",
    "        )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "\n",
    "\"\"\"Demo of using a chain as a custom action.\"\"\"\n",
    "config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "app = LLMRails(config)\n",
    "\n",
    "# Create and register the chain directly as an action.\n",
    "qa_chain = _get_qa_chain(app.llm)\n",
    "app.register_action(qa_chain, name=\"qa_chain\")\n",
    "\n",
    "# Change to mode here to experiment with the multiple ways of using the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "890183f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nemoguardrails.flows.runtime:Processing event: {'type': 'UtteranceUserActionFinished', 'final_transcript': 'What'}\n",
      "INFO:nemoguardrails.flows.runtime:Event :: UtteranceUserActionFinished {'final_transcript': 'What'}\n",
      "INFO:nemoguardrails.flows.runtime:Processing event: {'type': 'StartInternalSystemAction', 'uid': '43aa8063-4470-426c-b317-ab6dcb0dc8d8', 'event_created_at': '2023-10-28T13:11:54.259107+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'generate_user_intent', 'action_params': {}, 'action_result_key': None, 'action_uid': '801a2cda-d800-43e5-a446-c94f5d15099d', 'is_system_action': True}\n",
      "INFO:nemoguardrails.flows.runtime:Event :: StartInternalSystemAction {'uid': '43aa8063-4470-426c-b317-ab6dcb0dc8d8', 'event_created_at': '2023-10-28T13:11:54.259107+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'generate_user_intent', 'action_params': {}, 'action_result_key': None, 'action_uid': '801a2cda-d800-43e5-a446-c94f5d15099d', 'is_system_action': True}\n",
      "INFO:nemoguardrails.flows.runtime:Executing action :: generate_user_intent\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Executing registered action: generate_user_intent\n",
      "INFO:nemoguardrails.actions.llm.generation:Phase 1: Generating user intent\n",
      "INFO:chromadb.db.duckdb:Exiting: Cleaning up .chroma directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4162d4e7338d4fc78b47cdf70fb1e16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:nemoguardrails.logging.callbacks:Invocation Params :: {'model_name': 'text-davinci-003', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'request_timeout': None, 'logit_bias': {}, '_type': 'openai', 'stop': None}\n",
      "INFO:nemoguardrails.logging.callbacks:Prompt :: \"\"\"\n",
      "Below is a conversation between a helpful AI assistant and a user. The bot is designed to generate human-like text based on the input that it receives. The bot is talkative and provides lots of specific details. If the bot does not know the answer to a question, it truthfully says it does not know.\n",
      "\"\"\"\n",
      "\n",
      "# This is how a conversation between a user and the bot can go:\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"Tell me a bit about the history of NVIDIA.\"\n",
      "  ask general question\n",
      "bot response for general question\n",
      "  \"NVIDIA is a technology company that specializes in designing and manufacturing graphics processing units (GPUs) and other computer hardware. The company was founded in 1993 by Jen-Hsun Huang, Chris Malachowsky, and Curtis Priem.\"\n",
      "user \"tell me more\"\n",
      "  request more information\n",
      "bot provide more information\n",
      "  \"Initially, the company focused on developing 3D graphics processing technology for the PC gaming market. In 1999, NVIDIA released the GeForce 256, the world's first GPU, which was a major breakthrough for the gaming industry. The company continued to innovate in the GPU space, releasing new products and expanding into other markets such as professional graphics, mobile devices, and artificial intelligence.\"\n",
      "user \"thanks\"\n",
      "  express appreciation\n",
      "bot express appreciation and offer additional help\n",
      "  \"You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask.\"\n",
      "\n",
      "\n",
      "# This is how the user talks:\n",
      "user \"Who is the president?\"\n",
      "  ask off topic\n",
      "\n",
      "user \"Can you tell me a joke?\"\n",
      "  ask off topic\n",
      "\n",
      "user \"{intent:...}\"\n",
      "  ask off topic\n",
      "\n",
      "user \"hi\"\n",
      "  express greeting\n",
      "\n",
      "user \"You are stupid\"\n",
      "  express insult\n",
      "\n",
      "\n",
      "\n",
      "# This is the current conversation between the user and the bot:\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"What\"\n",
      "\n",
      "INFO:openai:message='OpenAI API response' path=https://api.openai.com/v1/completions processing_ms=1398 request_id=0a4a6f195aabf0751d7818659aa481e1 response_code=200\n",
      "INFO:nemoguardrails.logging.callbacks:Completion ::   ask for clarification\n",
      "\n",
      "bot provide clarification\n",
      "  \"I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences. Is there anything specific you would like me to help you with?\"\n",
      "INFO:nemoguardrails.logging.callbacks:Output Stats :: {'token_usage': {'completion_tokens': 57, 'prompt_tokens': 568, 'total_tokens': 625}, 'model_name': 'text-davinci-003'}\n",
      "INFO:nemoguardrails.logging.callbacks:--- :: LLM call took 1.66 seconds\n",
      "INFO:nemoguardrails.actions.llm.generation:Canonical form for user intent: ask for clarification\n",
      "INFO:nemoguardrails.flows.runtime:Processing event: {'type': 'UserIntent', 'uid': 'ebc9f9ca-dec0-406e-8362-3779439b9208', 'event_created_at': '2023-10-28T13:11:55.994191+00:00', 'source_uid': 'NeMoGuardrails', 'intent': 'ask for clarification'}\n",
      "INFO:nemoguardrails.flows.runtime:Event :: UserIntent {'uid': 'ebc9f9ca-dec0-406e-8362-3779439b9208', 'event_created_at': '2023-10-28T13:11:55.994191+00:00', 'source_uid': 'NeMoGuardrails', 'intent': 'ask for clarification'}\n",
      "INFO:nemoguardrails.flows.runtime:Processing event: {'type': 'StartInternalSystemAction', 'uid': '49a7ee45-6cf8-4d88-8417-f1c22d8385f6', 'event_created_at': '2023-10-28T13:11:55.995709+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'qa_chain', 'action_params': {'query': '$last_user_message'}, 'action_result_key': 'answer', 'action_uid': '823327ee-b8b1-4aeb-8d11-760568b733dd', 'is_system_action': False}\n",
      "INFO:nemoguardrails.flows.runtime:Event :: StartInternalSystemAction {'uid': '49a7ee45-6cf8-4d88-8417-f1c22d8385f6', 'event_created_at': '2023-10-28T13:11:55.995709+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'qa_chain', 'action_params': {'query': '$last_user_message'}, 'action_result_key': 'answer', 'action_uid': '823327ee-b8b1-4aeb-8d11-760568b733dd', 'is_system_action': False}\n",
      "INFO:nemoguardrails.flows.runtime:Executing action :: qa_chain\n",
      "INFO:nemoguardrails.actions.action_dispatcher:Executing registered action: qa_chain\n",
      "ERROR:nemoguardrails.actions.action_dispatcher:Error Missing some input keys: {'input_documents'} while execution qa_chain\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/macbookpro/opt/anaconda3/envs/python39/lib/python3.9/site-packages/nemoguardrails/actions/action_dispatcher.py\", line 144, in execute_action\n",
      "    result = await chain.arun(\n",
      "  File \"/Users/macbookpro/opt/anaconda3/envs/python39/lib/python3.9/site-packages/langchain/chains/base.py\", line 532, in arun\n",
      "    await self.acall(\n",
      "  File \"/Users/macbookpro/opt/anaconda3/envs/python39/lib/python3.9/site-packages/langchain/chains/base.py\", line 302, in acall\n",
      "    inputs = self.prep_inputs(inputs)\n",
      "  File \"/Users/macbookpro/opt/anaconda3/envs/python39/lib/python3.9/site-packages/langchain/chains/base.py\", line 389, in prep_inputs\n",
      "    self._validate_inputs(inputs)\n",
      "  File \"/Users/macbookpro/opt/anaconda3/envs/python39/lib/python3.9/site-packages/langchain/chains/base.py\", line 147, in _validate_inputs\n",
      "    raise ValueError(f\"Missing some input keys: {missing_keys}\")\n",
      "ValueError: Missing some input keys: {'input_documents'}\n",
      "INFO:nemoguardrails.flows.runtime:Processing event: {'type': 'hide_prev_turn'}\n",
      "INFO:nemoguardrails.flows.runtime:Event :: hide_prev_turn {}\n",
      "INFO:nemoguardrails.rails.llm.llmrails:--- :: Total processing took 1.74 seconds.\n",
      "INFO:nemoguardrails.rails.llm.llmrails:--- :: Stats: 1 total calls, 1.6573009490966797 total time, 625 total tokens, 568 total prompt tokens, 57 total completion tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I'm sorry, an internal error has occurred.\"}\n"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"What\"}\n",
    "]\n",
    "result = app.generate(messages=history)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d61dbb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    AI Response: AI Response: France is a country in western Europe. It has the world's sixth-largest economy and the fifth-largest population, with around 66 million people. Employment in France generally follows the same trends as other major European countries, with industries such as manufacturing, construction, and services being the major drivers of the economy.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is France\"\n",
    "\n",
    "relevant_docs = retriever.get_relevant_documents(\n",
    "            query\n",
    "        )\n",
    "\n",
    "response = qa_chain({\"input_documents\": relevant_docs, \n",
    "             \"human_input\": query}, \n",
    "            return_only_outputs=True)['output_text']\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088757d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
